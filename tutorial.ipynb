{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Med-BERT\n",
    "Step-by-step guide to the PyTorch implementation of [Med-BERT](https://www.nature.com/articles/s41746-021-00455-y) \n",
    "\n",
    "<sub><sup><em>\"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction.\" NPJ digital medicine 4.1 (2021): 1-13., Rasmy, Laila, et al. <em><sub><sup>\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of Med-Bert is to obtain good representations of elctronic health records (EHR) to make predicitons for downstream tasks. <br>\n",
    "In order to do so we leverage the power of the pretraining fine-tuning paradigm using a transformer architecture $^{1}$.  \n",
    "Originally used for Natural Language Processing, the transformers have proven their universality by showing SoTA results in fields like computer vision $^2$ and speech recognition $^3$. <br>\n",
    "Recently, a variant of the transformers, called BERT $^{4}$ has also been applied to medical data and electronic health records in particular $^{5-7}$.<br> \n",
    "There are countless tutorials that explain the theory and basic concepts behind the Transformers and BERT as well as their applicaiton to NLP, so here we will focus on using BERT for EHR specifically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "\n",
    "You can obtain synthetic data from: https://github.com/synthetichealth/synthea which can be processed (from csv) with https://github.com/kirilklein/ehr_preprocess.<br>\n",
    "This will produce a format ready to be turned into input features for our pipeline by main_data_pretrain.py (see data/raw/synthea500).<br>\n",
    "Here, we will start using the formatted data stored in: data/processed/pretrain/synthea500.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# used to create configs\n",
    "class DotDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with features stored in a dictionary. Inside that dictionary we have a list of lists, where every inner list represents one patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we can see the SNOMED-CT codes for patient number 4 in the validation set: \n",
      "['314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007', '314529007']\n"
     ]
    }
   ],
   "source": [
    "train_features = torch.load(\"data/processed/pretrain/synthea500/train.pt\")\n",
    "val_features = torch.load(\"data/processed/pretrain/synthea500/val.pt\")\n",
    "print(\"Here we can see the SNOMED-CT codes for patient number 4 in the validation set: \")\n",
    "print(val_features['concept'][4][:12]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we assign integers to each unique code and bring all sequences to the same length to pass data as a tensor.<br>\n",
    "Alternatively, we can use dynamic padding during training. The advantage of this is that we pad to the longest sequence in the batch instead of the longest sequence in the data (or truncation length). Because the sequence length contributes quadratically to training time this can accelerate training.<br>\n",
    "Additional tokens are needed to **mask** inputs and to take care of new codes that are not in the vocab dictionary that might appear in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding patients: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding patients: 391it [00:00, 2580.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  5,  6,  4,  6,  7,  4,  8,  9,  4,  9,  4,  7,  9,  4,  9,  6,  4,\n",
      "          9,  4,  9,  4,  9,  4,  9,  4,  9, 10,  4,  9,  4,  9,  4, 11,  9,  4,\n",
      "          9, 12,  6,  4, 13,  9, 10,  4,  9,  7,  4, 11,  9,  4,  6, 11,  9,  4,\n",
      "          9, 14,  4, 11,  7,  9,  4, 15, 16, 17, 18, 19,  4,  9,  6,  4, 11, 20,\n",
      "          9,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3, 21,  4,  5,  7,  4, 10,  7,  4, 22,  4,  7,  4, 23, 24, 20,  4, 25,\n",
      "          4, 26,  4,  6,  4, 27, 28, 29, 30,  4, 11, 27, 12,  4, 11, 27,  4, 11,\n",
      "         27,  4, 11, 27,  4, 27, 11,  6,  4, 11, 31, 27,  4, 11, 27,  4, 32, 33,\n",
      "         34,  4, 27, 12,  6,  4, 27, 35,  4, 36,  4, 37, 38, 39,  4, 27, 11,  4,\n",
      "         36,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3, 40,  9, 41, 42, 11, 43, 14,  4, 44,  9, 41, 42, 13,  6,  4, 31,  7,\n",
      "         11, 41, 42,  9, 44,  4,  9, 42, 41, 44, 45,  4, 13, 44, 41,  9, 42,  4,\n",
      "         44, 42, 41,  9, 11, 14,  6,  4, 42, 44, 41, 11,  9, 43,  4, 11, 44, 41,\n",
      "         42,  9,  7, 31,  4,  9, 44, 42, 41, 11,  6,  4, 11,  9, 44, 42, 41, 13,\n",
      "          4, 44, 41, 42,  9,  7, 11,  4, 44, 41, 42,  9, 11,  4, 44,  9, 41, 42,\n",
      "         46, 11,  4, 47, 48, 49, 50,  4, 51,  4]])\n"
     ]
    }
   ],
   "source": [
    "from medbert.features.tokenizer import EHRTokenizer\n",
    "tokenizer_config = DotDict({\n",
    "    'sep_tokens': True, # should we add [SEP] tokens?\n",
    "    'cls_token': True, # should we add a [CLS] token?\n",
    "    'padding': True, # should we pad the sequences?\n",
    "    'truncation': 100}) # how long should the longest sequence be\n",
    "tokenizer = EHRTokenizer(config=tokenizer_config)\n",
    "train_tokenized = tokenizer(train_features) \n",
    "print(train_tokenized['concept'][:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing the train set, one should freeze and save the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[MASK]': 1, '[UNK]': 2, '[CLS]': 3, '[SEP]': 4, '224299000': 5, '73595000': 6, '160903007': 7, '59621000': 8, '314076': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding patients: 112it [00:00, 4259.06it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.freeze_vocabulary()\n",
    "# tokenizer.save_vocab(\"../output/vocabulary.pt\")\n",
    "print({k:v for k,v in tokenizer.vocabulary.items() if v < 10})\n",
    "# Now we tokenize the validation set:\n",
    "val_tokenized = tokenizer(val_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset takes care of multiple things:\n",
    "* masking input tokens and creating an appropriate target\n",
    "* constructing an attention mask to ignore padding tokens \n",
    "* It allows the model to get an item (patient) via __getitem__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medbert.features import dataset\n",
    "\n",
    "dataset_config = DotDict({\n",
    "    'masked_ratio': 0.30, # 0.15 usually\n",
    "    'ignore_special_tokens': True,\n",
    "})\n",
    "train_dataset = dataset.MLM_PLOS_Dataset(train_tokenized, vocabulary=tokenizer.vocabulary, min_los=3,**dataset_config)\n",
    "val_dataset = dataset.MLM_PLOS_Dataset(val_tokenized, vocabulary=tokenizer.vocabulary,  min_los=3, **dataset_config) \n",
    "patient = train_dataset[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masking is adapted from BERT:\n",
    "* Select len(sequence)*masked_ratio tokens\n",
    "* Replace 80% of the selected tokens with 1 (mask token) \n",
    "* Replace 10% of the selected tokens with a random token\n",
    "* Keep the remaining 10% unchanged\n",
    "* The target consists of the original tokens in the place of the selected tokens. The rest is ignored, filling it with an ignore index e.g. -100\n",
    "\n",
    "When comparing the original tokenized features and the masked features, you can see that some integers were replaced by 1 (mask token). Others were replaced by random tokens and some are left unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  8,  7,  6,  9,  9, 10,  6,  9,  7, 11,  9, 11,  9,  9])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_mask = patient['target']!= -100 # mask\n",
    "train_tokenized['concept'][0][mask_mask] # original sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 64,   1,   7,   6,   1, 166,  67,   1,   1,   1,  11,  98,  11,   1,\n",
       "        134])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient['concept'][mask_mask] # masked sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  8,  7,  6,  9,  9, 10,  6,  9,  7, 11,  9, 11,  9,  9])\n"
     ]
    }
   ],
   "source": [
    "print(patient['target'][mask_mask]) # target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForPreTraining, BertConfig\n",
    "from torch.optim import AdamW\n",
    "\n",
    "num_attention_heads = 3\n",
    "hidden_size = num_attention_heads*10 # for parallel computation\n",
    "intermediate_size = hidden_size*4 # from original paper\n",
    "\n",
    "model = BertForPreTraining(\n",
    "        BertConfig(\n",
    "            vocab_size=len(train_dataset.vocabulary),\n",
    "            type_vocab_size=int(train_dataset.max_segments),\n",
    "            max_position_embeddings= 1024, # used for sequence embeddings\n",
    "            hidden_size= hidden_size,\n",
    "            num_hidden_layers= 3,\n",
    "            linear=True,\n",
    "            num_attention_heads= num_attention_heads,\n",
    "            intermediate_size= intermediate_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay= 0.01,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Run name not provided. Using random run name: 3c5b0d19972b43909606e7d8dc518858\n",
      "[INFO] Run folder: ../runs\\3c5b0d19972b43909606e7d8dc518858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0:  29%|██▊       | 2/7 [00:00<00:02,  2.42it/s, loss=6.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1: 6.631284713745117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0:  57%|█████▋    | 4/7 [00:01<00:01,  2.79it/s, loss=6.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2: 6.633287668228149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s, loss=6.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 3: 6.633130073547363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch 0 train loss: 5.685057844434466\n",
      "[INFO] Epoch 0 val loss: 6.625337839126587\n",
      "[INFO] Epoch 0 metrics: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1:  29%|██▊       | 2/7 [00:00<00:01,  3.27it/s, loss=6.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1: 6.62165904045105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1:  57%|█████▋    | 4/7 [00:01<00:00,  3.11it/s, loss=6.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 2: 6.615631818771362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|██████████| 7/7 [00:02<00:00,  3.36it/s, loss=6.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 3: 6.612935304641724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch 1 train loss: 5.671493189675467\n",
      "[INFO] Epoch 1 val loss: 6.6075599193573\n",
      "[INFO] Epoch 1 metrics: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from medbert.trainer import trainer\n",
    "import importlib\n",
    "importlib.reload(trainer)\n",
    "\n",
    "ehr_trainer = trainer.EHRTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    args=DotDict({\n",
    "        'epochs': 2,\n",
    "        'batch_size': 64,\n",
    "        'effective_batch_size': 128,\n",
    "    }),\n",
    "    cfg=DotDict({'scheduler': None,\n",
    "                 'collate_fn': None,\n",
    "                 'run_name':'ptest'}))\n",
    "ehr_trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<sub><sup>[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).</sub></sup> <br>\n",
    "<sub><sup>[2] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).</sub></sup> <br>\n",
    "<sub><sup>[3] Polyak, Adam, et al. \"Speech resynthesis from discrete disentangled self-supervised representations.\" arXiv preprint arXiv:2104.00355 (2021).</sub></sup><br>\n",
    "<sub><sup>[4] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).</sub></sup><br>\n",
    "<sub><sup>[5] Li, Yikuan, et al. \"BEHRT: transformer for electronic health records.\" Scientific reports 10.1 (2020): 1-12..</sub></sup><br>\n",
    "<sub><sup>[6] Shang, Junyuan, et al. \"Pre-training of graph augmented transformers for medication recommendation.\" arXiv preprint arXiv:1906.00346 (2019).</sub></sup><br>\n",
    "<sub><sup>[7] Pang, Chao, et al. \"CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks.\" Machine Learning for Health. PMLR, 2021.</sub></sup><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('medbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d17b095f833cb6f6ab98a8b16539b4f83b513338040a731dc495c8057944685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
